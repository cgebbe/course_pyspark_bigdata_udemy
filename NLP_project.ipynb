{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c983772a-f2d5-42b0-b4cd-37d00d25aa44",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0516d1c-3e05-488b-8e41-34f9767ea9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2b2cc2-09e6-43a9-b257-ecb3c5698a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\n",
    "    \"/home/jovyan/udemy_pyspark/resources/Spark_for_Machine_Learning/Natural_Language_Processing/smsspamcollection/SMSSpamCollection.csv\",\n",
    "    inferSchema=True,\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4add2b54-c354-48d9-8ac2-53184b49390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = (data\n",
    " .withColumnRenamed(\"_c0\",\"class\")\n",
    " .withColumnRenamed(\"_c1\",\"text\")\n",
    ")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a95fe3-5948-496e-b51b-b140a53b87cd",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fd0d6-29d0-484a-a207-41395c3917f4",
   "metadata": {},
   "source": [
    "## Lenght of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649b82d7-0d1e-446e-8f46-46d5d94f737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = data.withColumn(\"length\", length(data[\"text\"]))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e859f3c-4e63-436e-92da-532a83c91166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average length of spam is longer than of ham.\n",
    "data.groupby(\"class\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035e348-c7a9-4b50-8db8-65522fb82ed7",
   "metadata": {},
   "source": [
    "## More features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9195195b-46e5-4cb4-b3a6-beda84097f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    ")\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1342809a-6079-4ba9-bdc0-8e22a3137d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"token_text\"),\n",
    "    StopWordsRemover(inputCol=\"token_text\", outputCol=\"stop_token\"),\n",
    "    CountVectorizer(inputCol=\"stop_token\", outputCol=\"token_count\"),\n",
    "    IDF(inputCol=\"token_count\", outputCol=\"tf_idf\"),\n",
    "    VectorAssembler(inputCols=[\"tf_idf\",\"length\"],outputCol=\"features\"),\n",
    "\n",
    "    StringIndexer(inputCol=\"class\",outputCol=\"label\"),\n",
    "])\n",
    "\n",
    "# NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a79931-6975-4320-9b76-76a1b9d904f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- token_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stop_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_count: vector (nullable = true)\n",
      " |-- tf_idf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(class='ham', text='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', length=111, token_text=['go', 'until', 'jurong', 'point,', 'crazy..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'there', 'got', 'amore', 'wat...'], stop_token=['go', 'jurong', 'point,', 'crazy..', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'cine', 'got', 'amore', 'wat...'], token_count=SparseVector(13423, {7: 1.0, 11: 1.0, 31: 1.0, 61: 1.0, 72: 1.0, 344: 1.0, 625: 1.0, 731: 1.0, 1409: 1.0, 1598: 1.0, 4485: 1.0, 6440: 1.0, 8092: 1.0, 8838: 1.0, 11344: 1.0, 12979: 1.0}), tf_idf=SparseVector(13423, {7: 3.1126, 11: 3.2055, 31: 3.822, 61: 4.2072, 72: 4.322, 344: 5.4072, 625: 5.918, 731: 6.1411, 1409: 6.6801, 1598: 6.8343, 4485: 7.5274, 6440: 7.9329, 8092: 7.9329, 8838: 7.9329, 11344: 7.9329, 12979: 7.9329}), features=SparseVector(13424, {7: 3.1126, 11: 3.2055, 31: 3.822, 61: 4.2072, 72: 4.322, 344: 5.4072, 625: 5.918, 731: 6.1411, 1409: 6.6801, 1598: 6.8343, 4485: 7.5274, 6440: 7.9329, 8092: 7.9329, 8838: 7.9329, 11344: 7.9329, 12979: 7.9329, 13423: 111.0}), label=0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data=pipeline.fit(data).transform(data)\n",
    "feature_data.printSchema()\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff048c70-e06c-4ffe-9653-f21049fe532f",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0238d556-369e-4bc1-ac48-7e59a1bfcc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|\n",
      "|  0.0|(13424,[0,1,2,13,...|\n",
      "|  0.0|(13424,[0,1,2,41,...|\n",
      "|  0.0|(13424,[0,1,3,9,1...|\n",
      "|  0.0|(13424,[0,1,4,50,...|\n",
      "|  0.0|(13424,[0,1,7,8,1...|\n",
      "|  0.0|(13424,[0,1,7,8,1...|\n",
      "|  0.0|(13424,[0,1,7,15,...|\n",
      "|  0.0|(13424,[0,1,9,14,...|\n",
      "|  0.0|(13424,[0,1,9,14,...|\n",
      "|  0.0|(13424,[0,1,11,32...|\n",
      "|  0.0|(13424,[0,1,12,33...|\n",
      "|  0.0|(13424,[0,1,14,18...|\n",
      "|  0.0|(13424,[0,1,14,31...|\n",
      "|  0.0|(13424,[0,1,14,78...|\n",
      "|  0.0|(13424,[0,1,18,20...|\n",
      "|  0.0|(13424,[0,1,20,27...|\n",
      "|  0.0|(13424,[0,1,21,27...|\n",
      "|  0.0|(13424,[0,1,21,27...|\n",
      "|  0.0|(13424,[0,1,24,31...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = feature_data.select(\"label\",\"features\").randomSplit([0.7,0.3])\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a46bc2-4a55-4630-974e-1bbea48daa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes \n",
    "# use other models such as LogisticRegression or RandomForestClassifier\n",
    "\n",
    "model = NaiveBayes().fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b87a08-976b-419e-afab-b374405f1ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,5,15,...|[-1000.6230644835...|[1.0,7.2163496869...|       0.0|\n",
      "|  0.0|(13424,[0,1,5,20,...|[-807.17522539058...|[1.0,1.2502394538...|       0.0|\n",
      "|  0.0|(13424,[0,1,15,20...|[-666.05591848575...|[1.0,2.9806634934...|       0.0|\n",
      "|  0.0|(13424,[0,1,17,19...|[-806.20232700415...|[1.0,1.7927402686...|       0.0|\n",
      "|  0.0|(13424,[0,1,23,63...|[-1293.9083732758...|[1.0,2.4780250159...|       0.0|\n",
      "|  0.0|(13424,[0,1,31,43...|[-339.23325515474...|[1.0,1.7425569725...|       0.0|\n",
      "|  0.0|(13424,[0,1,72,10...|[-664.11081346361...|[1.0,8.7326595016...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-97.561866276162...|[0.99999998060230...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3301.1886267519...|[1.0,3.6339239551...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2505.5074041903...|[1.0,9.9318902949...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,40,...|[-1576.7012429977...|[0.99999995310537...|       0.0|\n",
      "|  0.0|(13424,[0,2,5,8,4...|[-832.67763412880...|[1.0,6.4112929313...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,8,1...|[-450.91779042980...|[1.0,1.6678593362...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-734.79139484184...|[1.0,5.0447471441...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-831.35346605550...|[1.0,6.5949311369...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,31,...|[-650.68485492850...|[1.0,3.3291171022...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,114...|[-455.74321832121...|[1.0,1.8227652330...|       0.0|\n",
      "|  0.0|(13424,[0,2,8,28,...|[-1310.5191837815...|[1.0,1.3471750510...|       0.0|\n",
      "|  0.0|(13424,[0,2,10,13...|[-5323.9348349382...|[1.0,8.2417057206...|       0.0|\n",
      "|  0.0|(13424,[0,2,10,13...|[-5323.9348349382...|[1.0,8.2417057206...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = model.transform(test_data)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91b9e8-dbe8-43fe-95f8-45dae84f8f30",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cf8dcbc-c53f-4c1f-9aae-f820774e8285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9219574751815383"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "MulticlassClassificationEvaluator(metricName=\"f1\").evaluate(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
